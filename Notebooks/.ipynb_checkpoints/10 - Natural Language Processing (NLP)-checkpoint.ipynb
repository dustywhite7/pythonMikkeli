{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8653a602",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937e2c81",
   "metadata": {},
   "source": [
    "## Defining Natural Language Processing\n",
    "\n",
    "As we continue further into our content this semester, you have probably noticed that each tool takes us to a new level of being able to conduct analysis with less code. As we progress, we move toward what are called \"higher level\" programming tools. Python itself is a high-level programming language, meaning that it is written in a way that is easier for humans to read than computers, with lots of translation happening behind the scenes. \n",
    "\n",
    "Tools like `pandas` remove a lot of the manual work from data processing. `numpy` and `scipy` handle much of the mathematical and statistical work that we want to do with our data. Natural Language Processing (NLP) is a tool similar to regex, but allowing us to take our text analytics to entirely new levels.\n",
    "\n",
    "Where regex allows us to look for text-based patterns in our words or string content, we want to go further. We want to look for **meaning**-based content. How do I find content that reflects anger? What words are most common in those contexts? What about when the content reflects joy? Sadness? Regex is not enough in these cases, and so we build to an even more powerful tool.\n",
    "\n",
    "NLP is a broad set of tools designed in order to enable users to work with text in ways that a human might work with text. When we work with text, we look for structures like sentences, and within those sentences we look for nouns to tell us who or what is the focus of the content. We look for verbs to understand what is happening. Adjectives and other descriptors help us to better understand the nuances of context. NLP models are trained to recognize these elements in text, and to be able to leverage that content to break text down and provide human users valuable information at a larger scale than would be possible if the document were simply read (slowly) by a human.\n",
    "\n",
    "| Regex | NLP |\n",
    "| --- | --- |\n",
    "| Create patterns to match in text | Identify the structure of text and use that to refine information|\n",
    "| Used to verify or find data | Used to analyze data |\n",
    "| Applies user-defined rules | Relies heavily on ML-based (or other) models |\n",
    "\n",
    "\n",
    "## What NLP can do\n",
    "\n",
    "So what can NLP do for us?\n",
    "\n",
    "### Identify parts of speech\n",
    "\n",
    "When we pass a document (really a string, but typically we provide a fairly large string to an NLP algorithm) to an NLP model, it is able to identify parts of speech (nouns, verbs, etc.). This enables us to quickly break down our text to find various kinds of keywords, and is the first step in many more complex pipelines.\n",
    "\n",
    "### Iterate over sentences\n",
    "\n",
    "NLP can identify sentences within a document. This makes for a powerful iterative tool, as we will be able to define a processing pipeline for each sentence, and then apply that pipeline to as many sentences as exist within our document, without having to write complex code to try and recognize where each sentence starts and ends (this is MUCH harder than it sounds).\n",
    "\n",
    "### Find words used to describe various nouns (or anything else!)\n",
    "\n",
    "Beyond simply identifying parts of speech, NLP models can be used to build a structural dependency tree of each sentence. This structure allows us to associate adjectives with their respective nouns, or adverbs to the verb that they modify. We can explore how various entities are described in our document based on word associations. We can even visualize the structure of the sentence using simple mapping functions.\n",
    "\n",
    "### Filter text for analysis\n",
    "\n",
    "We can use NLP to filter our text. We can look for sentences about a specific entity, or explore other ways of filtering our text in order to create a better understanding of overall patterns in the document.\n",
    "\n",
    "### Conduct sentiment analysis\n",
    "\n",
    "One of the most powerful capabilities resulting from NLP models is the ability to analyze sentiment within the text. Words have the ability to convey literal meanings, as well as the more subtle capacity to convey emotion. By looking for word combinations in our text, NLP models can provide sentiment measurements at the word, sentence, or document level. This provides the ability to sort through texts for specific sentiments to learn about the way in which emotion affects the outcomes we are examining in text. One example might be looking through descriptions for negative emotions, and being able to respond to unhappy customers by flagging negative descriptions.\n",
    "\n",
    "\n",
    "## Implementing NLP models\n",
    "\n",
    "To get started with NLP models, we need to install the right libraries (and a corpus!). The library that we will use is `spacy`, although there are multiple other options available to us. One other common NLP library is `nltk`, the Natural Language Toolkit (NLTK). In my experience NLTK is more commonly employed when someone wants to create their own model from scratch, rather than implement pre-built and optimized NLP models. \n",
    "\n",
    "In addition to a library to conduct NLP, we also rely on a **corpus**. A corpus is essentially a model of a specific language that is built to enable the actual analysis. `spacy` as a library is a general structure that can be implemented on ANY language. The corpus allows us to select a specific language, and a model of that language built on a specific set of information. \n",
    "\n",
    "When we install `spacy`, we will also download the `en_core_web_sm` corpus. This corpus is a small-sized model of the english language, and was trained on web-based data. That means that it will perform best on data drawn from websites, and is not as strong a model for other contexts (like analyzing Pride and Prejudice). That doesn't mean that we can't use in other contexts, but it does suggest that results will not be as refined or accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec470144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the SpaCy library\n",
    "\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b853fb08",
   "metadata": {},
   "source": [
    "Now we need to get ourselves some text to analyze before we jump into the NLP world. In true form, here is another favorite old book. We will take a look at the first three chapters of Jane Eyre from [Project Gutenberg](https://www.gutenberg.org/browse/scores/top)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80b7967",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "jane = requests.get(\n",
    "\"https://github.com/dustywhite7/Econ8320/raw/master/AssignmentData/janeEyreCh1to3.txt\"\n",
    ").text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4eaf65b",
   "metadata": {},
   "source": [
    "You can take a look at the text to get an idea of what we will be working with. Once you're ready, we will go ahead and import the `spacy` library. The very first thing we do once we import `spacy` is to load the coprus, so that we are able to use its language models to parse our document. Let's run the code, and then discuss what is happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8e1cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(jane)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49227892",
   "metadata": {},
   "source": [
    "### The structure of a parsed document\n",
    "\n",
    "When we create an `nlp` object based on our corpus, we are creating our pipeline for working with text. Our corpus contains all of the information necessary to prepare our data for analysis. When we create our `doc` object, we are passing our document through the processing pipeline. Our new parsed document (`doc` in this case), has some important **attributes**:\n",
    "\n",
    "- `sents` - a generator function to iterate over each sentence in the document\n",
    "- `token` - each individual element of the document\n",
    "    - Elements exist at the word/punctuation level\n",
    "\n",
    "After being processed, our document has been broken down into tokens, and then (to some extent) reconstructed into sentences. Additionally, each token is mapped out in relation to the other tokens within a sentence, and is described using various attributes to inform how that token relates to the text around it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5e2533",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [i.text.replace('\\n', ' ') for i in doc.sents][:10] # print first 10 sentences, replacing newlines with spaces\n",
    "\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179c2eb4",
   "metadata": {},
   "source": [
    "### Understanding tokens\n",
    "\n",
    "Recall that each word is represented as a token in the processed document. These tokens are immensely powerful. They are the word, but also more than that. Words in the English language are often modified based on context. Verbs are conjugated, nouns may be plural, among many possibilities. Each word is **tokenized** through our corpus in order to identify the underlying word.\n",
    "\n",
    "This is important, because we might want to look for each instance of a single word in our corpus. Let's say that we want to find every instance of \"eat\". If we look for \"eat\", we want to make sure that \"eats\" and \"ate\", as well as \"eating\" and other forms are all considered. This is where tokenization becomes critical. Each token contains the text value from the original document, but also the **lemmatized** word. The lemma is the base form of the word, allowing us to search for lemmas rather than the text word. This streamlines our ability to analyze text by focusing on lemmas rather than unprocessed text.\n",
    "\n",
    "Other valuable attributes are also associated with our tokens:\n",
    "- `lemma_` - the \"root word\" from which a token/word is derived\n",
    "- `pos_` - the part of speech of a token/word\n",
    "- `dep_` - the relationship of dependent tokens to the parent token (adjectives to nouns, etc.)\n",
    "- `like_email`/`like_num`/`like_url` - check if a token is like an email, number, or url (unlikely in Jane Eyre)\n",
    "\n",
    "Let's look at the first 100 non-space, non-punctuation lemmas in Jane Eyre:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575643ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas100 = [(i.lemma_, i.text) for i in doc if (not i.is_punct) and (not i.is_space)][:100]\n",
    "\n",
    "print(lemmas100) # The lemma comes first, followed by the actual word in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ab5d10",
   "metadata": {},
   "source": [
    "We can also filter words by part of speech using the `pos_` attribute of our tokens. Let's look for the first 100 nouns in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17feed3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns = [i.text for i in doc if i.pos_=='NOUN'][:100]\n",
    "\n",
    "print(nouns) # The lemma comes first, followed by the actual word in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8789bea",
   "metadata": {},
   "source": [
    "Pretty cool! If you look through that list, you'll see that there are a lot of different kinds of nouns. Two that stood out to me are \"drawing\" and \"room\"... oh wait... that's ONE NOUN that is two words! And leads us right into **noun chunks**. ;)\n",
    "\n",
    "Sometimes, you want to be able to see a \"complete\" noun, and noun chunks are the tool to use!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa6ced5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns = [i.text.replace('\\n', ' ') for i in doc.noun_chunks][:100] # getting rid of new lines in our noun chunks\n",
    "\n",
    "print(nouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07655fd0",
   "metadata": {},
   "source": [
    "That's better. Noun chunks include all of the modifiers for a given noun, and make it easier to build a more complete understanding of the references being made. Why do we care? Because \"the red moreen curtain\" is the object, and we want to be sure to understand the implication of the full object, rather than only the word within the noun chunk that is actually a noun.\n",
    "\n",
    "If we want to understand more about the nature of the relationships between words within a sentence, we can plot a dependency tree.\n",
    "\n",
    "**NOTE: When you run the following code, be sure to click the STOP button when you're done, or no other code will run! The renderer for the dependency tree will keep running until you terminate it!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbcc83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "sent = [i for i in doc.sents][100]\n",
    "displacy.serve(sent, style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10a962f",
   "metadata": {},
   "source": [
    "Using this mapping, we can see each of the clauses of the sentence, and how words within the sentence relate to one another. The mapping can help us to understand whether or not we have correctly identified parts of speech that are associated with the topics we are trying to uncover in our code.\n",
    "\n",
    "\n",
    "## Sentiment Analysis\n",
    "\n",
    "My personal favorite part of NLP, sentiment analysis is a very powerful instrument for understanding text and creating actionable items. Many firms use sentiment analysis in combination with their social media accounts to measure engagement and understand how successful marketing campaigns or other interactions are with target audiences.\n",
    "\n",
    "In order to conduct sentiment analysis using spacy, we are going to use a library called `spacytextblob`. This library includes supplemental material that expands the english corpus' ability to process our data. When utilized, `spacytextblob` is going to add sentiment analysis models to the general pipeline created through `spacy`. We can install the library with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127e7720",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pip install spacytextblob\n",
    "# Note: only use the `!` characters if installing from inside of jupyter notebook or other python interpreter (not when installing from shell/command prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc88de1",
   "metadata": {},
   "source": [
    "Next, we can incorporate the `SpacyTextBlob` process into our pipeline:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d796d33a",
   "metadata": {},
   "source": [
    "import spacy\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "import requests\n",
    "\n",
    "jane = requests.get(\n",
    "\"https://github.com/dustywhite7/Econ8320/raw/master/AssignmentData/janeEyreCh1to3.txt\"\n",
    ").text\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.add_pipe('spacytextblob')\n",
    "\n",
    "blob = nlp(jane)\n",
    "\n",
    "sents = [i for i in blob.sents]\n",
    "\n",
    "for sentence in sents[:10]:\n",
    "    print(\"Polarity: {0:3.2f}, Subjectivity: {1:3.2f}\".format(sentence._.polarity, sentence._.subjectivity))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594cb151",
   "metadata": {},
   "source": [
    "We can add code to our processing pipeline using the `nlp.add_pipe()` method on our `nlp` object. In this case, we are adding the sentiment analysis information created through `SpacyTextBlob`, but this can be literally anything. We can create any kind of function that we want to implement on our code, and can add that functionality to our pipeline in the same way. More examples are available in the `spacy` [documentation](https://spacy.io/usage/processing-pipelines#pipelines).\n",
    "\n",
    "If you want to get more in-depth with NLP, I highly recommend that you explore the course material available through `spacy`'s own NLP curriculum (freely available!): https://course.spacy.io/en/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ea49d9",
   "metadata": {},
   "source": [
    "## Solve it!\n",
    "\n",
    "In this project, you will use the text from chapters 44 and 45 of *Pride and Prejudice*. Please find the following information:\n",
    "\n",
    "- The number of sentences (store as `int` in a variable named `sentences`)\n",
    "- A list of all proper nouns used across the two chapters (stored as strings in the `names` variable)\n",
    "- A DataFrame containing a count of the top 20 adjectives used in the text (stored in the `adjectives` variable)\n",
    "    - Be sure to make all words lower case ONLY!\n",
    "- A bar chart of the top 20 adjectives used in the text\n",
    "    - Put this into the second graded cell\n",
    "    \n",
    "### NLP Code here $\\downarrow$$\\downarrow$$\\downarrow$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b45e1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
